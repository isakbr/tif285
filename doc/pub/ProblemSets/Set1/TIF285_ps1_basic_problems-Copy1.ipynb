{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Instructions\n",
    "- See deadline on the course web page\n",
    "- This problem set is performed individually.See examination rules on the course web page.\n",
    "- Students are allowed to discuss together and help each other when solving the problems. However, every student must understand and be able to explain his/her submitted solution. Plagiarism is not allowed (submissions will be both manually and automatically monitored).\n",
    "- The two notebooks for each problem set contain a number of basic and extra problems; you can choose which and how many to work on.\n",
    "- Many problems are automatically graded using `assert` statements. You should check that your code passes these statements without raising an `AssertionError`. Note that there might be additional, hidden tests that must be passed for full credit. In addition, some tasks are either completely manually graded or part-automatic/part-manual.\n",
    "- Note that grading is performed in the teacher's python environment, which is based on the conda `environment.yml` file in the course github repo. Please avoid using additional python modules (such as `plotly`) as this might cause automatic tests to fail.\n",
    "\n",
    "- **Important:** Hand-in is performed through the following actions:\n",
    "  - Make sure to always complete **Task 0** in the header part of the notebook. \n",
    "  - Upload your solution in the form of your edited version of this jupyter notebook via the appropriate module in Canvas.\n",
    "  - The name of the uploaded file **must be the same as the original one**!\n",
    "  \n",
    "  Note that the hand-in might not be automatically graded if you have changed the name of the uploaded file."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "- Make sure that the **run time is smaller than a few minutes**. If needed you might have to reduce some computational tasks; e.g. by decreasing the number of grid points or sampling steps. Please ask the supervisors if you are uncertain about the run time. \n",
    "\n",
    "- Your solutions are usually expected where it says `YOUR CODE HERE` or <font color=\"red\">\"PLEASE WRITE YOUR ANSWER HERE\"</font>."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 0 \n",
    "#### (0 points)\n",
    "Fill your personal details in the dictionary 'student' with the following key/value pairs:\n",
    "- **Lastname**: Your lastname as a string\n",
    "- **Firstname**: Your firstname as a string\n",
    "- **DOB-year**: The year for your date of birth as a four-digit integer\n",
    "- **DOB-month**: The month for your date of birth as an integer (1-12)\n",
    "- **DOB-day**: The year for your date of birth as an integer (1-31)\n",
    "- **CID**: Your Chalmers login ID as a string"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "student={}\n",
    "# Update the values below. Note the formats.\n",
    "student['Lastname']='Brundin'  # string\n",
    "student['Firstname']='Isak' # string\n",
    "student['CID']='isakbr'        # string\n",
    "student['DOB-year']=1997         # four-digit integer\n",
    "student['DOB-month']=9           # integer in the range [1, 12]\n",
    "student['DOB-day']=23             # integer in the range [1, 31]\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4b41fd8e57932c927ede3a155032c98",
     "grade": false,
     "grade_id": "student_info",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "for key in ['Lastname', 'Firstname','CID']:\n",
    "    assert type(student[key]) is str, f'{key} is wrong type.'\n",
    "    assert student[key] not in ['name_here','cid_here'],\\\n",
    "        f'Fill your {key} as a string.'\n",
    "\n",
    "for key in ['DOB-year', 'DOB-month','DOB-day']:\n",
    "    assert type(student[key]) is int, f'{key} is wrong type.'\n",
    "\n",
    "assert (student['DOB-year'] > 1900 and student['DOB-year'] < 2100)\n",
    "assert student['DOB-month'] in range(1,13), \\\n",
    "    'DOB month should be an integer in the range [1, 12]'\n",
    "assert student['DOB-day'] in range(1,32), \\\n",
    "    'DOB day should be an integer in the range [1, 31]'"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04fad85b593dac43e9a577b7ad0eda92",
     "grade": true,
     "grade_id": "correct_student_info",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem Set 1\n",
    "## Basic problems\n",
    "### Learning from data [TIF285], Chalmers, Fall 2021\n",
    "\n",
    "Last revised: 29-Aug-2021 by Christian Forssen [christian.forssen@chalmers.se]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 1\n",
    "### (1 point)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Installations\n",
    "Perform the installations and preparations that are described in the Getting Started instructions. At the end you should have:\n",
    "\n",
    "1. downloaded the current version of the course material from the github repository or from the course web page;\n",
    "2. a running python installation that includes the modules listed in the environment.yml file (e.g. numpy, matplotlib, pandas, emcee, scikit-learn, ...);\n",
    "3. been able to open and run the Jupyter Notebooks with the first week excercises.\n",
    "Ask the computer lab supervisors for assistance if needed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Module needed for grading tests\n",
    "import sys\n",
    "\n",
    "# Make sure that you are running python with version >= 3.x\n",
    "#\n",
    "# Import the following python modules with\n",
    "# the specified abreviations:\n",
    "# ---\n",
    "# numpy as np\n",
    "# scipy as scipy\n",
    "# matplotlib.pyplot as plt\n",
    "# pandas as pd\n",
    "# sklearn as skl\n",
    "# emcee as emcee\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import emcee"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9704f3e29d2d73008989e3a259d9a77d",
     "grade": false,
     "grade_id": "import",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "assert sys.version_info.major>=3, \\\n",
    "    'You are running Python version'+\\\n",
    "    f'{sys.version_info.major}.{sys.version_info.minor}'\n",
    "\n",
    "modules = [('numpy','np'), ('scipy', 'scipy'), \\\n",
    "           ('matplotlib.pyplot', 'plt'), ('pandas', 'pd'), \\\n",
    "           ('sklearn', 'skl'), ('emcee', 'emcee')]\n",
    "for (_module, _module_abbrev) in modules:\n",
    "    assert _module in sys.modules and _module_abbrev in dir(),\\\n",
    "        f'Module {_module} not loaded properly.'"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfee420a6338ea59afc21a227ee18577",
     "grade": true,
     "grade_id": "correct_import",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 2\n",
    "### (3 points)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generate data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Generate noisy data with a quadratic feature\n",
    "# use the following code:\n",
    "np.random.seed(42)\n",
    "m = 100 # Number of data\n",
    "\n",
    "# X are picked uniform random [0,2]\n",
    "X = 2 * np.random.rand(m, 1)\n",
    "# Linear relation to the predicted value, but with Gaussian noise (mean=0, variance=1)\n",
    "y = 0.5 * X**2 + X + 2 + 0.2 * np.random.randn(m, 1)"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47c6cb65850cb89a64545128c144f8a1",
     "grade": false,
     "grade_id": "cell-a0b708936a108258",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (a) Perform a linear regression using the Normal Equation\n",
    "Create the design matrix for a quadratic polynomial and solve the normal equation using matrix inversion."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def design_matrix(X, degree=2):\n",
    "    \"\"\"\n",
    "    Returns a design matrix.\n",
    "    \n",
    "    Args:\n",
    "        X: Array of shape (m,1) with 'm' independent data.\n",
    "        degree: Integer with the degree of the polynomial. \n",
    "                  Note that a degree-n polynomial has n+1 coefficients.\n",
    "                  \n",
    "    Returns:\n",
    "        X_d: Design matrix of shape (m, order+1).\n",
    "        \n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    X_d = np.zeros((m, degree+1))\n",
    "    for i in range(m):\n",
    "        for j in range(degree+1):\n",
    "            X_d[i,j] = X[i]**j\n",
    "    return X_d"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f281da58c59dffe144d18c64b30cf347",
     "grade": false,
     "grade_id": "design_matrix",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "assert design_matrix(X).shape == (len(X),3)\n",
    "assert design_matrix(X)[:,0].all() == 1\n",
    "assert design_matrix(X)[0,1] == X[0]\n",
    "assert design_matrix(X)[0,2] == X[0]**2"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2bf96c872bc417abf01f99b35e81db2",
     "grade": true,
     "grade_id": "correct_design_matrix",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def solve_normal_equation(X_d, y):\n",
    "    \"\"\"\n",
    "    Solve the normal equation.\n",
    "    \n",
    "    Args:\n",
    "        X_d: Design matrix of shape (m,n) with 'm' independent data\n",
    "               and 'n' features.\n",
    "        y: Dependent data of shape (m,1).\n",
    "                  \n",
    "    Returns:\n",
    "        theta_best: Best parameters, array of shape (n,).\n",
    "        \n",
    "    \"\"\"\n",
    "    X_t = np.transpose(X_d)\n",
    "    X_inv = np.linalg.inv(np.matmul(X_t,X_d))\n",
    "    theta_best = np.matmul(X_inv, X_t).dot(y).flatten()\n",
    "    return theta_best\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "350da970376594885f058c64685baf17",
     "grade": false,
     "grade_id": "solve_normal_equation",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "assert (solve_normal_equation(design_matrix(X), y)).shape==(3,),\\\n",
    "    'Return object has wrong shape. Maybe the `flatten` method will be useful?'"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11d49538f2f7f24f13547de02af90999",
     "grade": true,
     "grade_id": "correct_solve_normal_equation",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (b) Comparisons: \n",
    "- Print and compare the coefficients from the true data generator and the normal equation. \n",
    "- Plot the data and the model predictions in the same figure."
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6990966f05091b2c50afa83026318c34",
     "grade": false,
     "grade_id": "normal_equation_comparison",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Print and compare the coefficients from the true data generator and the normal equation.\n",
    "# Plot the data and the model predictions in the same figure.\n",
    "%matplotlib inline\n",
    "\n",
    "theta = solve_normal_equation(design_matrix(X), y)\n",
    "true_theta = [2,1,0.5]\n",
    "plt.plot(X,y,'k.', label = 'Data points')\n",
    "x_new = np.linspace(0,2,100)\n",
    "y_new = theta[0] + theta[1]*x_new+theta[2]*x_new**2\n",
    "plt.plot(x_new,y_new,'r', label = 'Model prediction')\n",
    "plt.legend()\n",
    "\n",
    "print(f'True parameters are: theta_0 = {str(true_theta[0])}, theta_1 = {str(true_theta[1])}, theta_2 = {str(true_theta[2])}')\n",
    "print(f'Fitted parameters are: theta_0 = {theta[0]:.{5}}, theta_1 = {theta[1]:.{5}}, theta_2 = {theta[2]:.{5}}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True parameters are: theta_0 = 2, theta_1 = 1, theta_2 = 0.5\n",
      "Fitted parameters are: theta_0 = 2.1123, theta_1 = 0.72778, theta_2 = 0.61621\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "827e5ed4af3034aac6b7259489b72b45",
     "grade": false,
     "grade_id": "cell-fbd1f3bdda876913",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 3\n",
    "### (3 points)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are three files in the directory `DataFiles`:\n",
    "- `dataset1.dat`\n",
    "- `dataset2.dat`\n",
    "- `dataset3.dat`\n",
    "\n",
    "Each data files contains two columns. The first column corresponds to the independent variables (the array X), and the second column corresponds to the dependent ones (the array y)."
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f523cfadaae0a17a63fbf834241101f",
     "grade": false,
     "grade_id": "cell-ad4ec88c445cd30c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# This cell is used in the solution notebook to generate the data. \n",
    "# It is hidden in the student version.\n",
    "# \n",
    "# Please ignore the comment in this cell that says \"YOUR CODE HERE\". It gets added automatically.\n",
    "# No solution code is needed here.\n",
    "# ---\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d584424c8353dc56c19697cb4b0f7e7",
     "grade": false,
     "grade_id": "cell-a51f823aed196b24",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (a) Implement linear regression and cost function\n",
    "- Load a data set and split it into 60% training and 40% validation data using the python function below.\n",
    "- Implement a linear regression function that takes training data as input and returns a best-fit parameter vector for a polynomial model of a specified degree.\n",
    "- Implement a cost function that takes data and model parameters as input and returns the mean-squared error."
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b56bb640833e47b3f518fce9c0008b4a",
     "grade": false,
     "grade_id": "cell-5f84c9fa34a2552c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# built-in convenience function for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(datafile, train_size=0.6):\n",
    "    \"\"\"\n",
    "    Reads data from file and returns training and validation sets.\n",
    "    \n",
    "    Args:\n",
    "        datafile: String with data filename path. The data file \n",
    "            should contain two columns: x, y\n",
    "        train_size: float indicating the fraction of training data\n",
    "            (default: 0.6)\n",
    "            \n",
    "    Returns:\n",
    "        (X_train, X_val, y_train, y_val): Tuple with four arrays \n",
    "            with training and validation data.\n",
    "    \"\"\"\n",
    "    X, y = np.loadtxt(datafile, unpack=True)\n",
    "    m = len(X)\n",
    "    X = X.reshape(m,1); y = y.reshape(m,1)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = \\\n",
    "        train_test_split(X, y, train_size=train_size, random_state=42)\n",
    "    return (X_train, X_val, y_train, y_val)"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b886fccdb06f514ef4e964cc065d9dc2",
     "grade": false,
     "grade_id": "cell-a53c70d002ece548",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# Implement a linear regression function that takes \n",
    "# training data as input and returns a best-fit parameter \n",
    "# vector for a polynomial model of a specified degree.\n",
    "\n",
    "def linear_regression(X, y, degree=2):\n",
    "    \"\"\"\n",
    "    Performs linear regression for a polynomial model.\n",
    "    \n",
    "    Args:\n",
    "        X: Array of shape (m,1) with 'm' independent data.\n",
    "        y: Array of shape (m,1) with 'm' dependent data.\n",
    "        degree: Integer with the degree of the polynomial. \n",
    "                  Note that a degree-n polynomial has n+1 coefficients.\n",
    "                  \n",
    "    Returns:\n",
    "        theta_fit: Best fit parameters. Array of shape (degree+1,)\n",
    "    \n",
    "    \"\"\"\n",
    "    X_mat = design_matrix(X,degree)\n",
    "    theta = solve_normal_equation(X_mat,y)\n",
    "    return theta\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5efdb5f19bfa224686f5f393f22e0fca",
     "grade": false,
     "grade_id": "cell-aa16bd9e1a61d370",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "datafile = 'DataFiles/dataset1.dat'\n",
    "(X_train, X_val, y_train, y_val) = load_data(datafile)\n",
    "degree = 20\n",
    "theta = linear_regression(X_train, y_train, degree=degree)\n",
    "plt.plot(X_train,y_train,'o')\n",
    "x_new = np.linspace(-3,3,len(X_train))\n",
    "y_new = theta[0]\n",
    "for i in range(degree):\n",
    "    y_new += x_new**(i+1)*theta[i+1]\n",
    "plt.plot(x_new,y_new)\n",
    "plt.show()"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "datafile = 'DataFiles/dataset1.dat'\n",
    "(X_train, X_val, y_train, y_val) = load_data(datafile)\n",
    "assert len(linear_regression(X_train, y_train, degree=3))==4\n",
    "a0_degree3fit_residual = \\\n",
    "    linear_regression(X_train, y_train, degree=3)[0] - 0.041779\n",
    "assert abs(a0_degree3fit_residual) < 1e-5\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "620e2ad58d6ef738fb5e6674245c4b2b",
     "grade": true,
     "grade_id": "cell-b26c3f6a60009e39",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# Implement a cost function that takes data and polynomial model \r\n",
    "# parameters as input and returns the mean-squared error.\r\n",
    "def mean_squared_error(X, y, theta):\r\n",
    "    \"\"\"\r\n",
    "    Compute the mean-squared error for data and a polynomial fit.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        X: Array of shape (m,1) with 'm' independent data.\r\n",
    "        y: Array of shape (m,1) with 'm' dependent data.\r\n",
    "        theta: Parameter array [shape (degree+1,)]. \r\n",
    "            The ordering corresponds to the constant term first.\r\n",
    "            \r\n",
    "    Return:\r\n",
    "        MSE (float): Mean-squared error defined as\r\n",
    "            MSE = (1/m) * sum_i (y[i] - y_model[i])**2,\r\n",
    "            where y_model[i] = \\sum_m theta[m]*X[i]**m \r\n",
    "    \"\"\"\r\n",
    "    m = len(X)\r\n",
    "    theta_index = np.linspace(0,len(theta)-1,len(theta))\r\n",
    "    y_model_i = np.array([np.sum([theta[int(the)]*x_i**the for the in theta_index]) for x_i in X.reshape(m,1)])\r\n",
    "    MSE = 1/m*np.sum((y.flatten()-y_model_i)**2)\r\n",
    "    return MSE"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d6e7a52e9f29ba4f3d25fe3a4e48b7e",
     "grade": false,
     "grade_id": "MSE",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Implement a function that takes data and polynomial model degree\r\n",
    "# and returns the mean-squared error for both training and validation data\r\n",
    "# as well as the best fit parameters.\r\n",
    "def polynomial_regression( data, degree):\r\n",
    "    \"\"\"\r\n",
    "    Calculates the mean square error for training- and validation data, aswell as best fittet\r\n",
    "    parameters for the training data.\r\n",
    "    \"\"\"\r\n",
    "    X_t, X_v, y_t, y_v = data\r\n",
    "    theta_t = linear_regression(X_t, y_t, degree)\r\n",
    "    MSE_t = mean_squared_error(X_t, y_t, theta_t)\r\n",
    "    MSE_v = mean_squared_error(X_v, y_v, theta_t)\r\n",
    "    return MSE_t, MSE_v, theta_t\r\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a6be1ce3357a1ab1bc035c4f7d1079b",
     "grade": false,
     "grade_id": "polynomial_regression",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "X_train=np.array([[1],[2],[3]])\r\n",
    "y_train=np.array([[2],[5],[10]])\r\n",
    "assert mean_squared_error(X_train, y_train, np.array([1,0,1]))==0\r\n",
    "assert mean_squared_error(X_train, y_train, np.array([0,0,0]))==43\r\n",
    "\r\n",
    "X_val = np.array([[4],[5]])\r\n",
    "y_val = np.array([[17],[27]])\r\n",
    "MSE_train, MSE_val, theta_fit = \\\r\n",
    "    polynomial_regression( (X_train, X_val, y_train, y_val), 2)\r\n",
    "assert MSE_val-0.5 < 1e-5\r\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dfd29e85e48a4738b69916b6796651c7",
     "grade": true,
     "grade_id": "correct_MSE",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (b) Perform linear regression with different polynomial models\n",
    "- For each data set you should then perform linear regression using polynomial models of order 1,2,3,4,5, and 20.\n",
    "- Finally, print the fit coefficients for each polynomial model that was considered and print also the mean-squared error (MSE) for both the training and the validation sets."
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "678e458083b6a9b788bb5b8a6eb4dd1e",
     "grade": false,
     "grade_id": "cell-0a736787d7c078d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "datafile1 = 'DataFiles/dataset1.dat'\n",
    "data_1 = load_data(datafile1)\n",
    "datafile2 = 'DataFiles/dataset2.dat'\n",
    "data_2 = load_data(datafile2)\n",
    "datafile3 = 'DataFiles/dataset3.dat'\n",
    "data_3 = load_data(datafile3)\n",
    "\n",
    "degrees = [1,2,3,4,5,20]\n",
    "frst_reg = list()\n",
    "scnd_reg = list()\n",
    "thrd_reg = list()\n",
    "for num in degrees:\n",
    "    frst_reg.append(polynomial_regression(data_1, num))\n",
    "    scnd_reg.append(polynomial_regression(data_2, num))\n",
    "    thrd_reg.append(polynomial_regression(data_3, num))\n",
    "for ind, reg in enumerate(frst_reg):\n",
    "    print((f'The parameters for the first dataset for polynomial model {degrees[ind]} is: \\n'\n",
    "          f'{reg[2]}, the MSE for training is {reg[0]} and for validation {reg[1]}\\n'))\n",
    "print('\\n')\n",
    "for ind, reg in enumerate(scnd_reg):\n",
    "    print((f'The parameters for the second dataset for polynomial model {degrees[ind]} is: \\n'\n",
    "          f'{reg[2]}, the MSE for training is {reg[0]} and for validation {reg[1]}\\n'))\n",
    "\n",
    "print('\\n')\n",
    "for ind, reg in enumerate(thrd_reg):\n",
    "    print((f'The parameters for the third dataset for polynomial model {degrees[ind]} is: \\n'\n",
    "          f'{reg[2]}, the MSE for training is {reg[0]} and for validation {reg[1]}\\n'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The parameters for the first dataset for polynomial model 1 is: \n",
      "[ 8.64449594 -1.23456593], the MSE for training is 58.06150271116937 and for validation 39.87895290105992\n",
      "\n",
      "The parameters for the first dataset for polynomial model 2 is: \n",
      "[ 0.12905012 -0.6366678   2.58692409], the MSE for training is 4.411974882287925 and for validation 4.117577224817619\n",
      "\n",
      "The parameters for the first dataset for polynomial model 3 is: \n",
      "[ 0.041779    1.26978243  2.52600994 -0.33172737], the MSE for training is 2.325816503967931 and for validation 2.544061720642741\n",
      "\n",
      "The parameters for the first dataset for polynomial model 4 is: \n",
      "[ 1.99354331  0.99062846  0.50868938 -0.24964384  0.24951078], the MSE for training is 0.00968506888827296 and for validation 0.005507913415150208\n",
      "\n",
      "The parameters for the first dataset for polynomial model 5 is: \n",
      "[ 1.99321069e+00  9.82284423e-01  5.09920902e-01 -2.45236024e-01\n",
      "  2.49294053e-01 -4.41040648e-04], the MSE for training is 0.00967031690314307 and for validation 0.0056187167672455424\n",
      "\n",
      "The parameters for the first dataset for polynomial model 20 is: \n",
      "[ 4.97126419e-01  1.05137454e+00  1.51207440e+00 -4.17859065e-01\n",
      " -1.50318286e+00  1.46325961e-01  2.70071920e+00 -1.31173604e-02\n",
      " -2.20718883e+00 -3.53716430e-02  1.01763540e+00  1.91408103e-02\n",
      " -2.80826399e-01 -4.52159307e-03  4.73660721e-02  5.61520370e-04\n",
      " -4.78281729e-03 -3.56915507e-05  2.65426807e-04  9.15370579e-07\n",
      " -6.22299327e-06], the MSE for training is 0.6544999209304657 and for validation 0.7334184273294085\n",
      "\n",
      "\n",
      "\n",
      "The parameters for the second dataset for polynomial model 1 is: \n",
      "[2.92526837 0.39697212], the MSE for training is 3.9992632486854363 and for validation 3.1512767998539752\n",
      "\n",
      "The parameters for the second dataset for polynomial model 2 is: \n",
      "[1.05194098 0.52850475 0.56910183], the MSE for training is 1.4028220490581405 and for validation 1.2782627069783474\n",
      "\n",
      "The parameters for the second dataset for polynomial model 3 is: \n",
      "[ 0.98685097  1.95040555  0.52366981 -0.24741449], the MSE for training is 0.2423493288343251 and for validation 0.13501288555560995\n",
      "\n",
      "The parameters for the second dataset for polynomial model 4 is: \n",
      "[ 0.96771657  1.95314228  0.5434469  -0.24821921 -0.00244611], the MSE for training is 0.24212672220682288 and for validation 0.1376978353787209\n",
      "\n",
      "The parameters for the second dataset for polynomial model 5 is: \n",
      "[ 0.96605343  1.91142211  0.54960451 -0.22618012 -0.00352974 -0.0022052 ], the MSE for training is 0.24175792257857756 and for validation 0.14046791918114143\n",
      "\n",
      "The parameters for the second dataset for polynomial model 20 is: \n",
      "[ 4.81219017e-01  2.08358410e+00  2.85233657e+00 -1.05037311e+00\n",
      " -8.32538918e+00  7.29565511e-01  1.34770868e+01 -6.58849953e-02\n",
      " -1.10374283e+01 -1.76760521e-01  5.08848510e+00  9.56961996e-02\n",
      " -1.40415242e+00 -2.26091070e-02  2.36830916e-01  2.80777707e-03\n",
      " -2.39140901e-02 -1.78467775e-04  1.32713411e-03  4.57710756e-06\n",
      " -3.11149640e-05], the MSE for training is 0.24196361413763653 and for validation 0.2530023273892402\n",
      "\n",
      "\n",
      "\n",
      "The parameters for the third dataset for polynomial model 1 is: \n",
      "[3.11283585 0.27668312], the MSE for training is 8.236642405144838 and for validation 5.977557499024456\n",
      "\n",
      "The parameters for the third dataset for polynomial model 2 is: \n",
      "[1.01045328 0.42429848 0.63868695], the MSE for training is 4.96644010756737 and for validation 3.8899784573856295\n",
      "\n",
      "The parameters for the third dataset for polynomial model 3 is: \n",
      "[ 0.94740386  1.80162221  0.59467924 -0.23965796], the MSE for training is 3.877589261349201 and for validation 2.1602061688897565\n",
      "\n",
      "The parameters for the third dataset for polynomial model 4 is: \n",
      "[ 0.87086627  1.81256911  0.6737876  -0.24287683 -0.00978446], the MSE for training is 3.8740275553091656 and for validation 2.2031653660595225\n",
      "\n",
      "The parameters for the third dataset for polynomial model 5 is: \n",
      "[ 0.86421373  1.64568846  0.69841804 -0.15472049 -0.01411894 -0.00882081], the MSE for training is 3.86812676125724 and for validation 2.2474867068982647\n",
      "\n",
      "The parameters for the third dataset for polynomial model 20 is: \n",
      "[-1.33409152e-01  2.30995548e+00  9.53606898e+00 -3.44529878e+00\n",
      " -3.32391664e+01  2.91797035e+00  5.39044016e+01 -2.63621832e-01\n",
      " -4.41498912e+01 -7.07022847e-01  2.03539805e+01  3.82783302e-01\n",
      " -5.61661226e+00 -9.04365593e-02  9.47323717e-01  1.12311311e-02\n",
      " -9.56563593e-02 -7.13872427e-04  5.30853637e-03  1.83084639e-05\n",
      " -1.24459854e-04], the MSE for training is 3.4028210922776165 and for validation 3.610809554660321\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "898c9652ff10c9817d46d55d5be266d9",
     "grade": false,
     "grade_id": "cell-49fbcaec1138af05",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the text cell below to answer the following two questions:\n",
    "- Which degrees of polynomials do you think was used for generating the different datasets?\n",
    "- Which dataset do you think has the most noise?\n",
    "\n",
    "Discuss your reasoning."
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca1f4ede86ea51e10818cfe3db423ff4",
     "grade": false,
     "grade_id": "cell-fcfe8b4f88e942d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Discussion***\n",
    "\n",
    "We can see in the data above that the lowest MSE(both training and validation) is achieved for polynomial 4 and 5. It is very similar values for polynomial 3 for dataset 2 and 3 but the polynomial values for dataset 1 is considerably larger.\n",
    "\n",
    "It is reasonable to assume that if the values for polynomial 4 and 5 are similiar the fit for 4 is better, since it will be less overfitted. We can also note that the fifth parameter for polynomial is very small for dataset 1, and comfortably the smallest for dataset 2 and 3. The other 4 parameters are very similiar, the same parity and agree to 1-2 decimals. Therefor the conclussion can be drawn that polynomial 5 is essentially polynomial 4 with and added small fifth parameter. This would explain the similar MSE values. The same argument can be made when comparing polynomil 3 to 4, similar values, small added fourth term, for dataset 2 and 3. Hence I think that for dataset 1 polynomial 4 was used and for dataset 2, 3 polynomial 3 was used.\n",
    "\n",
    "I think that the third dataset has the most noice. This is because no fit yields low MSE for any polynomial. Even for the presumed used polynomial 3 it still has the MSE for training is 4.1145 and for validation 1.9173. Compare this to the lowest MSE for dataset 1 at around 0.0113 and 0.00456 and for dataset 2 around 0.257 and 0.12 for training and validation respectively.\n"
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb4cda3ecfa02dd28b9121059f9511c5",
     "grade": true,
     "grade_id": "cell-93dac302fd63f96a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Problem 4\n",
    "### (3 points)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Standard medical example by applying Bayesian rules of probability"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose there is an unknown disease (call it UD) and there is a test for it.\n",
    "\n",
    "a. The false positive rate is 2.3%. (\"False positive\" means the test says you have UD, but you don't.) <br>\n",
    "b. The false negative rate is 1.4%. (\"False negative\" means you have UD, but the test says you don't.)\n",
    "\n",
    "Assume that 1 in 10,000 people have the disease. You are given the test and get a positive result.  Your ultimate goal is to find the probability that you actually have the disease. \n",
    "$% Some LaTeX definitions we'll use.\n",
    "\\newcommand{\\pr}{\\textrm{p}}\n",
    "$"
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33ae39785963e7ea72d0eb70d2103cd6",
     "grade": false,
     "grade_id": "cell-a717eb4d32ba845e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll do it using the Bayesian rules.\n",
    "\n",
    "We'll use the notation:\n",
    "\n",
    "* $H$ = \"you have UD\"\n",
    "* $\\overline H$ = \"you do not have UD\"  \n",
    "* $D$ = \"you test positive for UD\"\n",
    "* $\\overline D$ = \"you test negative for UD\"  "
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e3d6d1a8635628beb75d133afccc515",
     "grade": false,
     "grade_id": "cell-51752b16483bb655",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the text cell below to answer the following questions:\n",
    "<br>\n",
    "Notation: $H$ = \"you have UD\", &nbsp;&nbsp; $\\overline H$ = \"you do not have UD\",  &nbsp;&nbsp; $D$ = \"you test positive for UD\", &nbsp;&nbsp;  $\\overline D$ = \"you test negative for UD\" \n",
    "\n",
    "a. *Before doing a calculation (or thinking too hard :), does your intuition tell you the probability you have the disease is high or low?*\n",
    "<br>\n",
    "\n",
    "b. *In the $p(\\cdot | \\cdot)$ notation, what is your ultimate goal?*\n",
    "<br>\n",
    "\n",
    "c. *Express the false positive rate in $p(\\cdot | \\cdot)$ notation.* \\[Ask yourself first: what is to the left of the bar?\\]\n",
    "<br>\n",
    "\n",
    "d. *Express the false negative rate in $p(\\cdot | \\cdot)$ notation. By applying the sum rule, what do you also know? (If you get stuck answering the question, do the next part first.)* \n",
    "<br>\n",
    "\n",
    "e. *Should $p(D|H) + p(D|\\overline H) = 1$?\n",
    "    Should $p(D|H) + p(\\overline D |H) = 1$?\n",
    "    (Hint: does the sum rule apply on the left or right of the $|$?)*\n",
    "<br>\n",
    "\n",
    "f. *Apply Bayes' theorem to your result for your ultimate goal (don't put in numbers yet).\n",
    "   What other probabilities do we need?*\n",
    "<br>"
   ],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "761b695a589fdd6055969477da26236a",
     "grade": false,
     "grade_id": "cell-61c95058fe103533",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* * *\n",
    "**PLEASE WRITE YOUR ANSWER HERE** \n",
    "* * *\n",
    "a) high!\n",
    "\n",
    "b) $p(H|D)$\n",
    "\n",
    "c) $p(D|\\overline H)$\n",
    "\n",
    "d) $1-p(D|\\overline H)$\n",
    "\n",
    "e) $p(D|H) + p(D|\\overline H) \\neq 1$, $p(D|H) + p(\\overline D |H) = 1$\n",
    "\n",
    "f) $p(H|D) = \\frac{p(D|H)p(H)}{p(D)}$\n"
   ],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb1da5dddbf1953ecc29a49363a2c0f1",
     "grade": true,
     "grade_id": "cell-0221c5c91a887b41",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "# Please fill the probabilities as values for the \n",
    "# corresponding keys in the following dictionary.\n",
    "# p(H) = 1/10000\n",
    "# p(Hbar) = 1-1/10000\n",
    "# p(false pos) = 2.3/100\n",
    "# p(false neg) = 1.4/100\n",
    "medical_example_probabilities = {}\n",
    "medical_example_probabilities['p(D|Hbar)'] = 2.3/100\n",
    "medical_example_probabilities['p(Dbar|H)'] = 1.4/100\n",
    "medical_example_probabilities['p(D|H)'] = 1-1.4/100\n",
    "medical_example_probabilities['p(H,Hbar|D)'] = 0\n",
    "medical_example_probabilities['p(Hbar)'] = 1-1/10000\n",
    "medical_example_probabilities['p(D)'] = 0.0230963 #(1-1.4/100)*1/10000+2.3/100*(1-1/10000)#(1 - p(Dbar|H)p(H))+p(D|Hbar)p(Hbar)\n",
    "medical_example_probabilities['p(H|D)'] = 0.00426908206 # 1-1.4/100*1/10000/0.0230964 # (p(D|H)p(H))/p(D)\n",
    "\n",
    "# \n",
    "# YOUR CODE HERE\n",
    "# "
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c26443747ad7abc8240520552e29550",
     "grade": false,
     "grade_id": "medical_example",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "for key in ['p(D|Hbar)', 'p(Dbar|H)', 'p(D|H)']:\n",
    "    assert medical_example_probabilities[key] > 0.\n",
    "    assert medical_example_probabilities[key] < 1.\n",
    "    \n",
    "assert medical_example_probabilities['p(H,Hbar|D)'] <= 1.0\n"
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5319de57d7e765d7efebff3ab392c769",
     "grade": true,
     "grade_id": "correct_1_medical_example",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "for key in ['p(Hbar)', 'p(D)', 'p(H|D)']:\n",
    "    assert medical_example_probabilities[key] > 0.\n",
    "    assert medical_example_probabilities[key] < 1."
   ],
   "outputs": [],
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f767d6ab42f1573d59d64de9f207cec",
     "grade": true,
     "grade_id": "correct_2_medical_example",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}